name: Load RDF Data to Oxigraph

on:
  workflow_dispatch:

permissions:
  contents: write  # For `Process RSS Updates` step

jobs:
  load-oxigraph:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Docker
        uses: docker/setup-buildx-action@v3

      - name: Cache Oxigraph DB
        uses: actions/cache@v4
        with:
          path: oxigraph_db
          key: oxigraph-db-manual-v1
          restore-keys: |
            oxigraph-db-

      - name: Check if oxigraph_db exists
        id: check-db
        run: |
          if [ -d "oxigraph_db" ]; then
            echo "Database exists"
            echo "LOAD_MODE=partial" >> $GITHUB_ENV
          else
            echo "Database not found, creating new"
            mkdir -p oxigraph_db
            echo "LOAD_MODE=full" >> $GITHUB_ENV
          fi

      - name: Check if RSS updates CSV has rows
        if: env.LOAD_MODE == 'partial'
        run: |
          row_count=$(awk 'END{print NR}' ./data-raw/rss-updates.csv)
          if [ "$row_count" -le 1 ]; then
            echo "No RSS updates found, skipping partial load."
            echo "LOAD_MODE=none" >> $GITHUB_ENV
          fi

      - name: Cache Last-Modified header
        id: cache-modified-header
        if: env.LOAD_MODE != 'none'
        uses: actions/cache@v4
        with:
          path: .last_modified
          key: last-modified-header
          restore-keys: last-modified-header

      - name: Fetch Last-Modified header from RDF catalog
        if: env.LOAD_MODE != 'none'
        run: |
          last_modified=$(curl -sI https://www.gutenberg.org/cache/epub/feeds/rdf-files.tar.bz2 | grep -i "Last-Modified" | cut -d' ' -f2-)
          echo "Last-Modified: $last_modified"

          # Check if the last modified header exists
          if [ -f .last_modified ]; then
            previous_modified=$(cat .last_modified)
          else
            previous_modified=""
          fi

          # Save the new Last-Modified value if it has changed
          if [ "$last_modified" != "$previous_modified" ]; then
            echo "New version of rdf-files.tar.bz2 detected, will download."
            echo "$last_modified" > .last_modified
            echo "DOWNLOAD_REQUIRED=true" >> $GITHUB_ENV
          else
            echo "rdf-files.tar.bz2 has not changed."
            echo "DOWNLOAD_REQUIRED=false" >> $GITHUB_ENV
          fi

      - name: Download RDF catalog
        if: env.DOWNLOAD_REQUIRED == 'true'
        run: |
          mkdir -p pg_rdf_files/cache/epub
          curl -o rdf-files.tar.bz2 https://www.gutenberg.org/cache/epub/feeds/rdf-files.tar.bz2
          tar -xjf rdf-files.tar.bz2 -C ./pg_rdf_files

      - name: Run Oxigraph Docker container
        if: env.LOAD_MODE != 'none'
        run: |
          docker run -d --name oxigraph_server \
            -v ${{ github.workspace }}/oxigraph_db:/data \
            -p 7878:7878 oxigraph/oxigraph:latest
          sleep 10

      - name: Load all RDF files into DB
        if: env.LOAD_MODE == 'full'
        run: |
          source .github/scripts/load_rdf.sh

          RDF_DIR=./pg_rdf_files/cache/epub
          i=0
          for id in $(ls $RDF_DIR); do
            load_rdf $id
            i=$((i+1))
            if [ $((i % 1000)) -eq 0 ]; then
              echo "[$(date '+%H:%M')] Processed $i files."
            fi
          done

      - name: Load CSV-specified RDF files into DB
        if: env.LOAD_MODE == 'partial'
        run: |
          # Pull the latest version of rss-updates.csv
          git pull origin main

          source .github/scripts/load_rdf.sh

          RDF_DIR=./pg_rdf_files/cache/epub
          processed_ids=()

          # Loop through the ids in the CSV
          ids=$(awk -F, 'NR>1 {print $2}' ./data-raw/rss-updates.csv)
          for id in $ids; do
            load_rdf $id
            processed_ids+=($id)
          done

          # Re-pull the latest version of rss-updates.csv
          git pull origin main

          # Create a temporary CSV excluding processed rows
          awk -F, 'NR==1 {print $0; next} {for (i in ARGV) if ($2 == ARGV[i]) next} 1' "${processed_ids[@]}" ./data-raw/rss-updates.csv > ./data-raw/rss-updates_temp.csv

          # Replace the original CSV with the updated one
          mv ./data-raw/rss-updates_temp.csv ./data-raw/rss-updates.csv

          # Set flag to indicate update if changes were made
          if [[ $(git status --porcelain ./data-raw/rss-updates.csv) ]]; then
            echo "CSV_UPDATED=true" >> $GITHUB_ENV
          fi

      - name: Commit and push updated CSV
        if: env.CSV_UPDATED == 'true'
        run: |
          git config user.name "github-actions"
          git config user.email "github-actions@github.com"
          git add ./data-raw/rss-updates.csv
          git commit -m "Processed RSS updates and removed processed rows"
          git push origin main

      - name: Perform SPARQL query to count ebooks
        if: env.LOAD_MODE != 'none'
        run: |
          curl -G --data-urlencode 'query=SELECT (COUNT(*) AS ?total) WHERE { GRAPH <http://gutenberg.org/graph/catalog> { ?s a <http://www.gutenberg.org/2009/pgterms/ebook> } }' \
            http://localhost:7878/query
