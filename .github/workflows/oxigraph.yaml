name: Load RDF Data to Oxigraph

on:
  # Daily at 1am UTC
  schedule:
    - cron: '0 1 * * *'
  workflow_dispatch:
    inputs:
      run_extractions:
        type: boolean
        description: 'Run extractions'
        default: false

permissions:
  contents: write  # For `Commit and push updated CSV` step

jobs:
  load-oxigraph:
    runs-on: ubuntu-latest
    env:
      GITHUB_PAT: ${{ secrets.GH_PAT }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          ref: main
          token: ${{ env.GITHUB_PAT }}

      - name: Cache Oxigraph DB
        uses: actions/cache@v4
        with:
          path: oxigraph_db
          key: oxigraph-db-manual-v1
          restore-keys: |
            oxigraph-db-

      - name: Check if oxigraph_db exists
        id: check-db
        run: |
          if [ -d "oxigraph_db" ]; then
            echo "Database exists"
            echo "LOAD_MODE=partial" >> $GITHUB_ENV
          else
            echo "Database not found, creating new"
            mkdir -p oxigraph_db
            echo "LOAD_MODE=full" >> $GITHUB_ENV
          fi

      - name: Check if RSS updates CSV has rows
        if: env.LOAD_MODE == 'partial'
        run: |
          row_count=$(awk 'END{print NR}' ./data-raw/rss-updates.csv)
          if [ "$row_count" -le 1 ]; then
            echo "No RSS updates found, skipping partial load."
            echo "LOAD_MODE=none" >> $GITHUB_ENV
          fi

      - name: Set up Docker
        if: env.LOAD_MODE != 'none' || github.event.inputs.run_extractions == 'true'
        uses: docker/setup-buildx-action@v3

      - name: Download RDF catalog
        if: env.LOAD_MODE != 'none'
        run: |
          mkdir -p pg_rdf_files/cache/epub
          curl -o rdf-files.tar.bz2 https://www.gutenberg.org/cache/epub/feeds/rdf-files.tar.bz2
          tar -xjf rdf-files.tar.bz2 -C ./pg_rdf_files

      - name: Run Oxigraph Docker container
        if: env.LOAD_MODE != 'none' || github.event.inputs.run_extractions == 'true'
        run: |
          docker run -d --name oxigraph_server \
            -v ${{ github.workspace }}/oxigraph_db:/data \
            -p 7878:7878 oxigraph/oxigraph:latest
          until curl -s http://localhost:7878; do
            echo "Waiting for Oxigraph server to start..."
            sleep 5
          done

      - name: Load all RDF files into DB
        if: env.LOAD_MODE == 'full'
        run: |
          source .github/scripts/load_rdf.sh

          RDF_DIR=./pg_rdf_files/cache/epub
          i=0
          for id in $(ls $RDF_DIR); do
            load_rdf $id
            i=$((i+1))
            if [ $((i % 1000)) -eq 0 ]; then
              echo "[$(date '+%H:%M')] Processed $i files."
            fi
          done

      - name: Load CSV-specified RDF files into DB
        if: env.LOAD_MODE == 'partial'
        run: |
          # Pull the latest version of rss-updates.csv
          git pull origin main

          source .github/scripts/load_rdf.sh

          RDF_DIR=./pg_rdf_files/cache/epub
          processed_ids=()

          # Loop through the ids in the CSV
          ids=$(awk -F, 'NR>1 {print $2}' ./data-raw/rss-updates.csv)
          for id in $ids; do
            load_rdf $id
            processed_ids+=($id)
          done

          # Re-pull the latest version of rss-updates.csv
          git pull origin main

          # Create a temporary CSV excluding processed rows
          awk -F, -v ids="${processed_ids[*]}" '
          BEGIN {
            split(ids, arr, " ")
          }
          NR==1 {print $0; next}
          {
            for (i in arr) {
              if ($2 == arr[i]) next
            }
            print $0
          }' ./data-raw/rss-updates.csv > ./data-raw/rss-updates_temp.csv

          # Replace the original CSV with the updated one
          mv ./data-raw/rss-updates_temp.csv ./data-raw/rss-updates.csv

          # Set flag to indicate update if changes were made
          if [[ $(git status --porcelain ./data-raw/rss-updates.csv) ]]; then
            echo "CSV_UPDATED=true" >> $GITHUB_ENV
          fi

      - name: Commit and push updated CSV
        if: env.CSV_UPDATED == 'true'
        run: |
          git config user.name "github-actions"
          git config user.email "github-actions@github.com"
          git add ./data-raw/rss-updates.csv
          git commit -m "ðŸ¤– Processed RSS updates and removed processed rows"
          git push

      - name: Perform SPARQL query to count ebooks
        if: env.LOAD_MODE != 'none' || github.event.inputs.run_extractions == 'true'
        run: |
          curl -G --data-urlencode 'query=SELECT (COUNT(*) AS ?total) WHERE { GRAPH <http://gutenberg.org/graph/catalog> { ?s a <http://www.gutenberg.org/2009/pgterms/ebook> } }' \
            http://localhost:7878/query

      - name: Extract authors
        if: env.LOAD_MODE != 'none' || github.event.inputs.run_extractions == 'true'
        run: ./.github/scripts/extract_gutenberg_authors.sh
